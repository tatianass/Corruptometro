library(rpart.plot)
# Fonts (Letras)
library(extrafont)
# dcast e melt
library(reshape2)
# Nuvens de palavras
library(wordcloud)
# Claster hierarquico
library(cluster)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
#diretorio com arquivo
train_file <- "/home/jefferson/Base de dados para food Recommendation/Yummly/train.json"
# importando o arquivo de um json para um data frame. O parametro flatten é utilizado para agrupar os dados aninhados
train <- fromJSON(train_file, flatten = TRUE)
# ----------- Bibliotecas INICIO
# Utilizada para importar o arquivo no formato JSON
library(jsonlite)
# Para gráficos
library(ggplot2)
#Para manipulação dos dados
library(dplyr)
# Utilizada para criar e tratar os corpus
# Utilizado para mineração de texto
library(tm)
# Gráfico das tabelas
library(knitr)
# Utilizada para criar as partições de treino e teste
library(caret)
# Utilizado para implementar o algoritmo de arvore de decisão (Implementa arvores de regressão)
library(rpart)
# Gráficos para árvores
library(rpart.plot)
# Fonts (Letras)
library(extrafont)
# dcast e melt
library(reshape2)
# Nuvens de palavras
library(wordcloud)
# Claster hierarquico
library(cluster)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
#diretorio com arquivo
train_file <- "/home/jefferson/Base de dados para food Recommendation/Yummly/train.json"
# importando o arquivo de um json para um data frame. O parametro flatten é utilizado para agrupar os dados aninhados
train <- fromJSON(train_file, flatten = TRUE)
# ----------- DATASET FIM
# Remover o id acredito não ser necessário
train <- select(train, -id)
# Apresntação gráfica da quantidade de registro com receitas de cada país
ggplot(train, aes(reorder(cuisine,cuisine, function(x)-length(x)))) + geom_bar() + theme_light() + theme(axis.text.x = element_text(angle = 60, hjust = 1, colour = "black"), text=element_text(size=20, family="serif", colour = "black")) +
xlab("Países") +
ylab("Quantidade de Receitas")
#+
#        ggtitle("Quantidade de Receitas por País")
# Quantidade de receitas
a <- filter(train, cuisine == "italian")
length(a$cuisine)
a <- filter(train, cuisine == "brazilian")
length(a$cuisine)
# Corpus para o treinamento. Utilizando apenas os ingredientes.
ingredientes <- Corpus(VectorSource(train$ingredients))
ingredientes
## O corpus utilizado contém 39.774 receitas de diferentes países
# O corpus é feito com os ingredientes de cada receita. Porém as vezes, nas receitas, pode acontecer de o mesmo ingrediente ser escrito de diferentes formas, e.g. simgular e plural. A função tm_map, do pacote tm, unifica as palavras com o mesmo radical, assim se uma receita tem o ingrediente "ovo" e na outra "ovos", estes serão reconhecidos com o mesmo ingrediente. Aqui foi utilizado a função stemDocument, mas também pode ser utilizado as funções: tolower, removePunctuation, removeWords, stopwords, removeNumbers
# Remover Pontuação
ingredientes <- tm_map(ingredientes, removePunctuation)
# Remover números
ingredientes <- tm_map(ingredientes, removeNumbers)
#Colocar todas as palavras em caixa baixa
ingredientes <- tm_map(ingredientes, content_transformer(tolower))
# Remover palavras desintereçantes com (or, and, of, the)
ingredientes <- tm_map(ingredientes, removeWords, stopwords("english"))
# Verificar palavras semelhantes (mesmo radical)
ingredientes <- tm_map(ingredientes, stemDocument)
# Remover espaços em branco
ingredientes <- tm_map(ingredientes, stripWhitespace)
# -------------------------------- Teste ___________________________________
#
ingredientes <- tm_map(ingredientes, PlainTextDocument)
# Criando uma matrix com os termos
dtm <- DocumentTermMatrix(ingredientes)
ingredientesDTM <- DocumentTermMatrix(ingredientes)
ingredientesDTM
ingredientesDTM <- as.data.frame(as.matrix(ingredientesDTM))
baseTreino <- createDataPartition(ingredientesDTM, p = 0.8, list = FALSE)
baseTreino <- createDataPartition(ingredientesDTM[1:1000], p = 0.8, list = FALSE)
baseTreino <- createDataPartition(ingredientesDTM[1:2], p = 0.8, list = FALSE)
baseTreino <- createDataPartition(ingredientesDTM[1:10], p = 0.8, list = FALSE)
# ----------- Bibliotecas INICIO
# Utilizada para importar o arquivo no formato JSON
library(jsonlite)
# Para gráficos
library(ggplot2)
#Para manipulação dos dados
library(dplyr)
# Utilizada para criar e tratar os corpus
# Utilizado para mineração de texto
library(tm)
# Gráfico das tabelas
library(knitr)
# Utilizada para criar as partições de treino e teste
library(caret)
# Utilizado para implementar o algoritmo de arvore de decisão (Implementa arvores de regressão)
library(rpart)
# Gráficos para árvores
library(rpart.plot)
# Fonts (Letras)
library(extrafont)
# dcast e melt
library(reshape2)
# Nuvens de palavras
library(wordcloud)
# Claster hierarquico
library(cluster)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
#diretorio com arquivo
train_file <- "/home/jefferson/Base de dados para food Recommendation/Yummly/train.json"
# importando o arquivo de um json para um data frame. O parametro flatten é utilizado para agrupar os dados aninhados
train <- fromJSON(train_file, flatten = TRUE)
# ----------- DATASET FIM
ingredientes <- Corpus(VectorSource(train$ingredients))
# Remover Pontuação
ingredientes <- tm_map(ingredientes, removePunctuation)
# Remover números
ingredientes <- tm_map(ingredientes, removeNumbers)
#Colocar todas as palavras em caixa baixa
ingredientes <- tm_map(ingredientes, content_transformer(tolower))
# Remover palavras desintereçantes com (or, and, of, the)
ingredientes <- tm_map(ingredientes, removeWords, stopwords("english"))
# Verificar palavras semelhantes (mesmo radical)
ingredientes <- tm_map(ingredientes, stemDocument)
# Remover espaços em branco
ingredientes <- tm_map(ingredientes, stripWhitespace)
train <- select(train, -cuisine)
ingredientes <- tm_map(ingredientes, removePunctuation)
# Remover números
ingredientes <- tm_map(ingredientes, removeNumbers)
#Colocar todas as palavras em caixa baixa
ingredientes <- tm_map(ingredientes, content_transformer(tolower))
# Remover palavras desintereçantes com (or, and, of, the)
ingredientes <- tm_map(ingredientes, removeWords, stopwords("english"))
# Verificar palavras semelhantes (mesmo radical)
ingredientes <- tm_map(ingredientes, stemDocument)
# Remover espaços em branco
ingredientes <- tm_map(ingredientes, stripWhitespace)
ingredientesDTM <- DocumentTermMatrix(ingredientes)
ingredientesDTM <- as.data.frame(as.matrix(ingredientesDTM))
ingredientesDTM$id <- as.factor(train$id)
baseTreino <- createDataPartition(y = ingredientesDTM$id, p = 0.8, list = FALSE)
treino <- ingredientesDTM[baseTreino,]
teste <- ingredientesDTM[-baseTreino,]
set.seed(9345)
modeloAjustado <- rpart(id ~ ., data = treino, method = "class")
modeloAjustado <- rpart( ~ ., data = treino, method = "class")
# ----------- Bibliotecas INICIO
# Utilizada para importar o arquivo no formato JSON
library(jsonlite)
# Para gráficos
library(ggplot2)
#Para manipulação dos dados
library(dplyr)
# Utilizada para criar e tratar os corpus
# Utilizado para mineração de texto
library(tm)
# Gráfico das tabelas
library(knitr)
# Utilizada para criar as partições de treino e teste
library(caret)
# Utilizado para implementar o algoritmo de arvore de decisão (Implementa arvores de regressão)
library(rpart)
# Gráficos para árvores
library(rpart.plot)
# Fonts (Letras)
library(extrafont)
# dcast e melt
library(reshape2)
# Nuvens de palavras
library(wordcloud)
# Claster hierarquico
library(cluster)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
#diretorio com arquivo
train_file <- "/home/jefferson/Base de dados para food Recommendation/Yummly/train.json"
# importando o arquivo de um json para um data frame. O parametro flatten é utilizado para agrupar os dados aninhados
train <- fromJSON(train_file, flatten = TRUE)
# ----------- DATASET FIM
train <- select(train, -id)
train <- select(train, -id)
ingredientes <- tm_map(ingredientes, removePunctuation)
# Remover números
ingredientes <- tm_map(ingredientes, removeNumbers)
#Colocar todas as palavras em caixa baixa
ingredientes <- tm_map(ingredientes, content_transformer(tolower))
# Remover palavras desintereçantes com (or, and, of, the)
ingredientes <- tm_map(ingredientes, removeWords, stopwords("english"))
# Verificar palavras semelhantes (mesmo radical)
ingredientes <- tm_map(ingredientes, stemDocument)
# Remover espaços em branco
ingredientes <- tm_map(ingredientes, stripWhitespace)
ingredientesDTM <- DocumentTermMatrix(ingredientes)
ingredientesDTM
ingredientesDTM <- as.data.frame(as.matrix(ingredientesDTM))
# A função as.factor() transforma um vetor em um fator. Adicionando a variavel com os nomes dos países
ingredientesDTM$cuisine <- as.factor(train$cuisine)
# criando a partição, com 80% para treino. Partição baseada nas receitas e list = false, para não verificar cada cozinha com todas os ingredientes em todas as execuções.
baseTreino <- createDataPartition(y = ingredientesDTM$cuisine, p = 0.8, list = FALSE)
# Recebe todos os elementos que estiverem dento da partição de treino"anova", "poisson", "class" or "exp"
treino <- ingredientesDTM[baseTreino,]
# Reebe todos os elementos que estiverem fora da partição de treino, ou seja, na partição de teste
teste <- ingredientesDTM[-baseTreino,]
### Utilizando o algoritimo de árvore de decisão
# Utilizado para forçar que os valores gerados aleatoriamente sejam sempre os mesmos valores.
set.seed(9345)
# Árvore de decisão. 1º parametro a formula utilizada que no caso será as receitas[cuisine](paises) por todas as variáveis. o 2º parametro é a partição de treino e o 3º parametro é o método utilizado ele pode ser: "anova", "poisson", "class" or "exp". Aqui será utilizado "class"" porque a base de dados é um fator.
modeloAjustado <- rpart(cuisine ~ ., data = treino, method = "class")
# Plotar o gráfico com as decisões
prp(modeloAjustado)
## Teste do modelo
# A função predict verifica o ajuste do modelo com base nos dados de teste. O parametrp neudata é a partição de teste e o type tipo da predição
testeModelo <- predict(modeloAjustado, newdata = teste, type = "class")
# Verifica as predições com o real valor
testeModeloCM <- confusionMatrix(testeModelo, teste$cuisine)
testeModeloCM
save(testeModeloCM)
library(xlsReadWrite)
install.packages("xlsReadWrite")
library(xlsReadWrite)
library(xlsReadWrite)
library(xlsx)
install.packages("xlsx")
library(xlsx)
install.packages("xlsx")
library(xlsx)
write.xls(testeModeloCM, "/home/jefferson/arquivo.xls")
library(xlsx)
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
testeModeloCM
write.csv(resultados, file = "/home/jefferson/aditivosGestaoSave.csv")
write.csv(aditivosGestaoSave, file = "/home/jefferson/aditivosGestaoSave.csv")
# ----------- Bibliotecas INICIO
# Utilizada para importar o arquivo no formato JSON
library(jsonlite)
# Para gráficos
library(ggplot2)
#Para manipulação dos dados
library(dplyr)
# Utilizada para criar e tratar os corpus
# Utilizado para mineração de texto
library(tm)
# Gráfico das tabelas
library(knitr)
# Utilizada para criar as partições de treino e teste
library(caret)
# Utilizado para implementar o algoritmo de arvore de decisão (Implementa arvores de regressão)
library(rpart)
# Gráficos para árvores
library(rpart.plot)
# Fonts (Letras)
library(extrafont)
# dcast e melt
library(reshape2)
# Nuvens de palavras
library(wordcloud)
# Claster hierarquico
library(cluster)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
codigo_ugestora <- "/home/jefferson/DataSet HackFest/HackFest/CSV/codigo_ugestora.csv"
aditivos <- "/home/jefferson/DataSet HackFest/HackFest/CSV/aditivos.csv"
contratos <- "/home/jefferson/DataSet HackFest/HackFest/CSV/contratos.csv"
quantidadeEleitores <- "/home/jefferson/DataSet HackFest/quantidade_de_eleitores_por_município.csv"
codigo_ugestora <- read.csv(codigo_ugestora)
aditivos = read.csv(aditivos)
contratos = read.csv(contratos)
quantidadeEleitores = read.csv(quantidadeEleitores)
# ----------- DATASET FIM
aditivosGestao <- group_by(aditivos, cd_UGestora) %>% mutate (quantidaTotalAditivo = length(vl_Aditivo))
aditivosGestao <- group_by(aditivosGestao, cd_UGestora, dt_Ano) %>% mutate(quantidadeAditivoPorAno = length(vl_Aditivo))
pregaoGestao <- filter(contratos, tp_Licitacao == 10 | tp_Licitacao == 11 | tp_Licitacao == 0)
pregaoGestao = group_by(pregaoGestao, cd_UGestora) %>% mutate(quantidadePregao = length(cd_UGestora))
pregaoGestao = group_by(pregaoGestao, cd_UGestora, dt_Ano) %>% mutate(quantidadeAnoPregao = length(cd_UGestora))
pregaoGestaoConvite <- filter(contratos, tp_Licitacao == 3)
pregaoGestaoConvite = group_by(pregaoGestaoConvite, cd_UGestora, dt_Ano) %>% mutate(quantidadeAnoConvite = length(cd_UGestora))
pregaoGestaoConvite = group_by(pregaoGestaoConvite, cd_UGestora) %>% mutate(quantidadeConvite = length(cd_UGestora))
quantidadeEleitores <- group_by(quantidadeEleitores, Abrang.ncia) %>% mutate(media = (Quantidade.2009 + Quantidade.2013)/2)
aditivosGestaoSave <- select(aditivosGestao, cd_UGestora, quantidaTotalAditivo, quantidadeAditivoPorAno, dt_Ano)
pregaoGestaoSave <- select(pregaoGestao, dt_Ano, cd_UGestora, quantidadePregao, quantidadeAnoPregao)
pregaoGestaoConviteSave <- select(pregaoGestaoConvite, dt_Ano, cd_UGestora, quantidadeConvite, quantidadeAnoConvite)
quantidadeEleitoresSave <- quantidadeEleitores
write.csv(aditivosGestaoSave, file = "/home/jefferson/aditivosGestaoSave.csv")
write.csv(pregaoGestaoSave, file = "/home/jefferson/pregaoGestaoSave.csv")
write.csv(pregaoGestaoConviteSave, file = "/home/jefferson/pregaoGestaoConviteSave.csv")
write.csv(quantidadeEleitoresSave, file = "/home/jefferson/quantidadeEleitoresSave.csv")
quantidadeEleitores <- "/home/jefferson/DataSet HackFest/quantidade_de_eleitores_por_município.csv"
quantidadeEleitoresSave <- quantidadeEleitores
quantidadeEleitores = read.csv(quantidadeEleitores)
quantidadeEleitoresSave <- quantidadeEleitores
View(quantidadeEleitoresSave)
View(aditivosGestaoSave)
View(pregaoGestaoConviteSave)
View(pregaoGestaoSave)
View(quantidadeEleitoresSave)
# ----------- Bibliotecas INICIO
# Utilizada para importar o arquivo no formato JSON
library(jsonlite)
# Para gráficos
library(ggplot2)
#Para manipulação dos dados
library(dplyr)
# Utilizada para criar e tratar os corpus
# Utilizado para mineração de texto
library(tm)
# Gráfico das tabelas
library(knitr)
# Utilizada para criar as partições de treino e teste
library(caret)
# Utilizado para implementar o algoritmo de arvore de decisão (Implementa arvores de regressão)
library(rpart)
# Gráficos para árvores
library(rpart.plot)
# Fonts (Letras)
library(extrafont)
# dcast e melt
library(reshape2)
# Nuvens de palavras
library(wordcloud)
# Claster hierarquico
library(cluster)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
codigo_ugestora <- "/home/jefferson/DataSet HackFest/HackFest/CSV/codigo_ugestora.csv"
aditivos <- "/home/jefferson/DataSet HackFest/HackFest/CSV/aditivos.csv"
contratos <- "/home/jefferson/DataSet HackFest/HackFest/CSV/contratos.csv"
quantidadeEleitores <- "/home/jefferson/DataSet HackFest/quantidade_de_eleitores_por_município.csv"
codigo_ugestora <- read.csv(codigo_ugestora)
aditivos = read.csv(aditivos)
contratos = read.csv(contratos)
quantidadeEleitores = read.csv(quantidadeEleitores)
# ----------- DATASET FIM
aditivosGestao <- group_by(aditivos, cd_UGestora) %>% mutate (quantidaTotalAditivo = length(vl_Aditivo))
aditivosGestao <- group_by(aditivosGestao, cd_UGestora, dt_Ano) %>% mutate(quantidadeAditivoPorAno = length(vl_Aditivo))
dispencaGestao <- filter(contratos, tp_Licitacao == 6 | tp_Licitacao == 7)
dispencaGestao = group_by(dispencaGestao, cd_UGestora) %>% mutate(quantidadePregao = length(cd_UGestora))
dispencaGestao = group_by(dispencaGestao, cd_UGestora, dt_Ano) %>% mutate(quantidadeAnoPregao = length(cd_UGestora))
GestaoConvite <- filter(contratos, tp_Licitacao == 3)
gestaoConvite = group_by(gestaoConvite, cd_UGestora, dt_Ano) %>% mutate(quantidadeAnoConvite = length(cd_UGestora))
gestaoConvite = group_by(gestaoConvite, cd_UGestora) %>% mutate(quantidadeConvite = length(cd_UGestora))
quantidadeEleitores <- group_by(quantidadeEleitores, Abrang.ncia) %>% mutate(media = (Quantidade.2009 + Quantidade.2013)/2)
gestaoConvite = group_by(gestaoConvite, cd_UGestora, dt_Ano) %>% mutate(quantidadeAnoConvite = length(cd_UGestora))
GestaoConvite <- filter(contratos, tp_Licitacao == 3)
View(GestaoConvite)
gestaoConvite <- filter(contratos, tp_Licitacao == 3)
gestaoConvite = group_by(gestaoConvite, cd_UGestora, dt_Ano) %>% mutate(quantidadeAnoConvite = length(cd_UGestora))
gestaoConvite = group_by(gestaoConvite, cd_UGestora) %>% mutate(quantidadeConvite = length(cd_UGestora))
quantidadeEleitores <- group_by(quantidadeEleitores, Abrang.ncia) %>% mutate(media = (Quantidade.2009 + Quantidade.2013)/2)
aditivosGestaoSave <- select(aditivosGestao, cd_UGestora, quantidaTotalAditivo, quantidadeAditivoPorAno, dt_Ano)
pregaoGestaoSave <- select(pregaoGestao, dt_Ano, cd_UGestora, quantidadePregao, quantidadeAnoPregao)
dispencaGestaoSave <- select(dispencaGestao, dt_Ano, cd_UGestora, quantidadePregao, quantidadeAnoPregao)
gestaoConviteSave <- select(gestaoConvite, dt_Ano, cd_UGestora, quantidadeConvite, quantidadeAnoConvite)
quantidadeEleitoresSave <- quantidadeEleitores
write.csv(aditivosGestaoSave, file = "/home/jefferson/aditivosGestaoSave.csv")
write.csv(pregaoGestaoSave, file = "/home/jefferson/pregaoGestaoSave.csv")
write.csv(dispencaGestaoSave, file = "/home/jefferson/pregaoGestaoSave.csv")
write.csv(dispencaGestaoSave, file = "/home/jefferson/dispencaGestaoSave.csv")
write.csv(gestaoConviteSave, file = "/home/jefferson/gestaoConviteSave.csv")
write.csv(quantidadeEleitoresSave, file = "/home/jefferson/quantidadeEleitoresSave.csv")
aditivosGestaoSave$dt_Ano[aditivosGestaoSave$dt_Ano == c(2009, 2010, 2011, 2012)] = 2010
View(aditivosGestaoSave)
aditivosGestaoSave$dt_Ano[aditivosGestaoSave$dt_Ano is c(2009, 2010, 2011, 2012)] = 2010
a <- merge(aditivosGestaoSave, dispencaGestaoSave, gestaoConviteSave, by = cd_UGestora)
a <- merge(aditivosGestaoSave, dispencaGestaoSave, gestaoConviteSave, by = aditivosGestaoSave$cd_UGestora)
View(gestaoConviteSave)
View(aditivosGestaoSave)
View(aditivosGestaoSave)
aditivosGestaoSave = unique(aditivosGestaoSave)
dispencaGestaoSave = unique(dispencaGestaoSave)
gestaoConviteSave = unique(gestaoConviteSave)
quantidadeEleitoresSave = unique(quantidadeEleitoresSave)
write.csv(aditivosGestaoSave, file = "/home/jefferson/aditivosGestaoSave.csv")
write.csv(dispencaGestaoSave, file = "/home/jefferson/dispencaGestaoSave.csv")
write.csv(gestaoConviteSave, file = "/home/jefferson/gestaoConviteSave.csv")
write.csv(quantidadeEleitoresSave, file = "/home/jefferson/quantidadeEleitoresSave.csv")
View(aditivosGestaoSave)
View(dispencaGestaoSave)
aditivosGestaoSave2009 = filter(aditivosGestaoSave, dt_Ano <= 2012)
aditivosGestaoSave2009$dt_Ano = 2009
aditivosGestaoSave2013 = filter(aditivosGestaoSave, dt_Ano >= 2013)
aditivosGestaoSave2013$dt_Ano = 2013
AaditivosGestaoSave = merge(aditivosGestaoSave2009, aditivosGestaoSave2013)
AaditivosGestaoSave = merge(aditivosGestaoSave2009, aditivosGestaoSave2013, by = )
AaditivosGestaoSave = merge(aditivosGestaoSave2009, aditivosGestaoSave2013)
source('~/ProjetosGit/Corruptometro/scripts/R/prediction.R')
source("data.R")
setwd("~/ProjetosGit/Corruptometro/scripts/R")
source('~/ProjetosGit/Corruptometro/scripts/R/prediction.R')
data = read.csv("../../data/tre_sagres_unificado.csv",header=FALSE,skip=1)
features = select(data, V6,V7,V8)
table(data$V5)
train_idx = createDataPartition(y=data$V5, p=.9,list=FALSE)
train = data[train_idx,]
test = data[-train_idx,]
features = select(train, V6,V7,V8)
prop.table(table(train$V5))
prop.table(table(test$V5))
grid = expand.grid(.ntree=c(10,20,30,40,50,100,200),.mtry=2,.model="tree")
fitControl = trainControl(method="repeatedcv",number=10,repeats=10,returnResamp="all")
labels = as.factor(train$V5)
model = train(x=features,y=labels,trControl=fitControl)
prob = predict
plot(model)
test_labels = as.factor(test$V5)
predictions = predict(model,newdata=test)
prob = predict(model,newdata=test,type = "prob")
confusionMatrix(data = predictions, test_labels)
data = read.csv("../../data/tre_sagres_unificado.csv",header=FALSE,skip=1)
View(data)
data2 = read.csv("../../data/tre_sagres_unificado.csv")
View(data2)
data = read.csv("../../data/tre_sagres_unificado.csv")
View(data)
data2 = read.csv("../../data/tre_sagres_unificado.csv")
View(data2)
features = select(data, V6,V7,V8)
data = read.csv("../../data/tre_sagres_unificado.csv",header=FALSE,skip=1)
features = select(data, V6,V7,V8)
table(data$V5)
prop.table(table(train$V5))
train_idx = createDataPartition(y=data$V5, p=.9,list=FALSE)
# Ccnjunto de treino
train = data[train_idx,]
# Conjunto de teste
test = data[-train_idx,]
# features do conjunto de treino
features = select(train, V6,V7,V8)
# # features do conjunto de treino
features
prop.table(table(train$V5))
prop.table(table(test$V5))
prop.table(table(train$V5))
prop.table(table(test$V5))
View(data)
grid = expand.grid(.ntree=c(10,20,30,40,50,100,200),.mtry=2,.model="tree")
grid
# ----------- Bibliotecas INICIO
# Utilizada para importar o arquivo no formato JSON
library(jsonlite)
# Para gráficos
library(ggplot2)
#Para manipulação dos dados
library(dplyr)
# Utilizada para criar e tratar os corpus
# Utilizado para mineração de texto
library(tm)
# Gráfico das tabelas
library(knitr)
# Utilizada para criar as partições de treino e teste
library(caret)
# Utilizado para implementar o algoritmo de arvore de decisão (Implementa arvores de regressão)
library(rpart)
# Gráficos para árvores
library(rpart.plot)
# ----------- Bibliotecas  FIM
# ----------- DATASET INICIO
#diretorio com arquivo
train_file <- "/home/jefferson/Base de dados para food Recommendation/Yummly/train.json"
# importando o arquivo de um json para um data frame. O parametro flatten é utilizado para agrupar os dados aninhados
train <- fromJSON(train_file, flatten = TRUE)
# ----------- DATASET FIM
train <- select(train, -id)
ingredientes <- Corpus(VectorSource(train$ingredients))
ingredientes
ingredientes <- tm_map(ingredientes, stemDocument)
ingredientes
# Construir uma matriz com os ingredientes existentes
ingredientesDTM <- DocumentTermMatrix(ingredientes)
ingredientesDTM
## Com o resultado posso observar que a receita que mais tem ingredientes tem 19 elementos
# A função sparse <- removeSparseTerms() abaixo remove todas os ingredientes que não aparecerem em pelo menos 1% das receitas. Ela não será utilizada pois acredito que estes ingredientes podem ter uma maior significancia ao indicar a receita.
# sparse <- removeSparseTerms(ingredientsDTM, 0.99)
# as.data.frame() Verifica se o objeto é um data frame ou o converte em um, se possível
# as.matrix() Cria uma matriz com os elementos
ingredientesDTM <- as.data.frame(as.matrix(ingredientesDTM))
# A função as.factor() transforma um vetor em um fator. Adicionando a variavel com os nomes dos países
ingredientesDTM$cuisine <- as.factor(train$cuisine)
# criando a partição, com 80% para treino. Partição baseada nas receitas e list = false, para não verificar cada cozinha com todas os ingredientes em todas as execuções.
baseTreino <- createDataPartition(y = ingredientesDTM$cuisine, p = 0.8, list = FALSE)
# Recebe todos os elementos que estiverem dento da partição de treino"anova", "poisson", "class" or "exp"
treino <- ingredientesDTM[baseTreino,]
# Reebe todos os elementos que estiverem fora da partição de treino, ou seja, na partição de teste
teste <- ingredientesDTM[-baseTreino,]
set.seed(9345)
# Árvore de decisão. 1º parametro a formula utilizada que no caso será as receitas[cuisine](paises) por todas as variáveis. o 2º parametro é a partição de treino e o 3º parametro é o método utilizado ele pode ser: "anova", "poisson", "class" or "exp". Aqui será utilizado "class"" porque a base de dados é um fator.
modeloAjustado <- rpart(cuisine ~ ., data = treino, method = "class")
# Plotar o gráfico com as decisões
prp(modeloAjustado)
testeModelo <- predict(modeloAjustado, newdata = teste, type = "class")
# Verifica as predições com o real valor
testeModeloCM <- confusionMatrix(testeModelo, teste$cuisine)
testeModeloCM
